{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from multiprocessing import Pool\n",
    "#from functools import partial\n",
    "import numpy as np\n",
    "#from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: loss of least square regression and binary logistic regression\n",
    "'''\n",
    "    pred() takes GBDT/RF outputs, i.e., the \"score\", as its inputs, and returns predictions.\n",
    "    g() is the gradient/1st order derivative, which takes true values \"true\" and scores as input, and returns gradient.\n",
    "    h() is the heassian/2nd order derivative, which takes true values \"true\" and scores as input, and returns hessian.\n",
    "'''\n",
    "class leastsquare(object):\n",
    "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
    "    def pred(self,score):\n",
    "        return score\n",
    "\n",
    "    def g(self,true,score):\n",
    "        return 2 * (score - true)\n",
    "\n",
    "    def h(self,true,score):\n",
    "        return 2 * np.ones_like(score)\n",
    "\n",
    "class logistic(object):\n",
    "    '''Loss class for log loss. As for log loss, pred function is logistic transformation.'''\n",
    "    def pred(self,score):\n",
    "        return 1 / (1 + np.exp(-score))\n",
    "\n",
    "    def g(self,true,score):\n",
    "        return self.pred(score) - true\n",
    "\n",
    "    def h(self,true,score):\n",
    "        return self.pred(score) * (1 - self.pred(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of a node on a tree\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp \n",
    "mp.set_start_method('fork')\n",
    "\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    Data structure that are used for storing a node on a tree.\n",
    "    \n",
    "    A tree is presented by a set of nested TreeNodes,\n",
    "    with one TreeNode pointing two child TreeNodes,\n",
    "    until a tree leaf is reached.\n",
    "    \n",
    "    A node on a tree can be either a leaf node or a non-leaf node.\n",
    "    '''\n",
    "    \n",
    "    #TODO\n",
    "    # def __init__(self, X, y, depth):\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None, depth=None, is_leaf=False):\n",
    "        # store essential information in every tree node\n",
    "        self.is_leaf = False if (feature is not None and threshold is not None) else True\n",
    "        self.feature = feature  # feature index for split\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        self.depth = depth \n",
    "        self.is_leaf = is_leaf\n",
    "\n",
    "\n",
    "\n",
    "# TODO: class of single tree\n",
    "class Tree(object):\n",
    "    '''\n",
    "    Class of a single decision tree in GBDT\n",
    "\n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        max_depth: The maximum depth of the tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf prediction, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule,\n",
    "            rf = 0 means we are training a GBDT.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_threads = None, \n",
    "                 max_depth = 5, min_sample_split = 5,\n",
    "                 lamda = 2, gamma = 1, rf = 0.5, ):\n",
    "        self.n_threads = n_threads\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.int_member = 0\n",
    "        self.multiproc = True if n_threads is not None and n_threads > 1 else False\n",
    "\n",
    "    def _leaf_value(self, g, h):\n",
    "        den = h.sum() + self.lamda\n",
    "        return -g.sum() / den if den > 0 else 0.0\n",
    "\n",
    "    def fit(self, train, g, h):\n",
    "        '''\n",
    "        train is the training data matrix, and must be numpy array (an n_train x m matrix).\n",
    "        g and h are gradient and hessian respectively.\n",
    "        '''\n",
    "        #TODO\n",
    "        # Initialize the root node\n",
    "        root = self.construct_tree(train, g, h)\n",
    "        # Set the root node to the tree\n",
    "        self.root = root\n",
    "        return self\n",
    "\n",
    "    def predict(self,test):\n",
    "        '''\n",
    "        test is the test data matrix, and must be numpy arrays (an n_test x m matrix).\n",
    "        Return predictions (scores) as an array.\n",
    "        '''\n",
    "        #TODO\n",
    "        result = np.zeros(test.shape[0])\n",
    "        # Traverse the tree for each test sample\n",
    "        for i in range(test.shape[0]):\n",
    "            node = self.root\n",
    "            while not node.is_leaf:\n",
    "                if test[i, node.feature] <= node.threshold:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            result[i] = node.value\n",
    "        # Return the predictions\n",
    "        return result\n",
    "\n",
    "    def construct_tree(self, X, g, h, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively build one tree in GBDT.\n",
    "        Stops when:\n",
    "        1. depth >= self.max_depth\n",
    "        2. number of samples < self.min_sample_split\n",
    "        3. best gain <= 0\n",
    "        \"\"\"\n",
    "        # 1. Stopping condition: too deep or too few samples\n",
    "        n_samples = X.shape[0]\n",
    "        if depth >= self.max_depth or n_samples < self.min_sample_split:\n",
    "            leaf_val = self._leaf_value(g, h)\n",
    "            return TreeNode(value=leaf_val, depth=depth, is_leaf=True)\n",
    "\n",
    "        # 2. Find best split\n",
    "        feat, thr, gain = self.find_best_decision_rule(X, g, h)\n",
    "        if gain is None or gain <= 0:\n",
    "            leaf_val = self._leaf_value(g, h)\n",
    "            return TreeNode(value=leaf_val, depth=depth, is_leaf=True)\n",
    "\n",
    "        # 3. Split data\n",
    "        mask_left = X[:, feat] <= thr\n",
    "        mask_right = ~mask_left\n",
    "\n",
    "        # 4. Ensure no empty or undersized child\n",
    "        if mask_left.sum() < self.min_sample_split or mask_right.sum() < self.min_sample_split:\n",
    "            leaf_val = self._leaf_value(g, h)\n",
    "            return TreeNode(value=leaf_val, depth=depth, is_leaf=True)\n",
    "\n",
    "        # 5. Recurse\n",
    "        left  = self.construct_tree(X[mask_left],  g[mask_left],  h[mask_left],  depth+1)\n",
    "        right = self.construct_tree(X[mask_right], g[mask_right], h[mask_right], depth+1)\n",
    "\n",
    "        # 6. Return internal node\n",
    "        return TreeNode(\n",
    "            feature=feat,\n",
    "            threshold=thr,\n",
    "            left=left,\n",
    "            right=right,\n",
    "            depth=depth,\n",
    "            is_leaf=False\n",
    "        )\n",
    "\n",
    "\n",
    "    def find_best_decision_rule(self, train, g, h):\n",
    "        '''\n",
    "        Return the best decision rule [feature, treshold], i.e., $(p_j, \\tau_j)$ on a node j, \n",
    "        train is the training data assigned to node j\n",
    "        g and h are the corresponding 1st and 2nd derivatives for each data point in train\n",
    "        g and h should be vectors of the same length as the number of data points in train\n",
    "        \n",
    "        for each feature, we find the best threshold by find_threshold(),\n",
    "        a [threshold, best_gain] list is returned for each feature.\n",
    "        Then we select the feature with the largest best_gain,\n",
    "        and return the best decision rule [feature, treshold] together with its gain.\n",
    "        '''\n",
    "        #TODO\n",
    "        # Version without multiprocessing.\n",
    "        # n_features = np.arange(train.shape[1])\n",
    "        # f_size = int(n_features.size * self.rf)\n",
    "        # feats = np.random.choice(n_features, size=f_size, replace=False) if self.rf > 0 else n_features\n",
    "        # best_gain = -np.inf\n",
    "        # best_feat = None\n",
    "        # best_thr = None\n",
    "        # threshold = None\n",
    "        # for f in feats:\n",
    "        #     # Find the best threshold and gain for feature f\n",
    "        #     threshold, gain = self.find_threshold(g, h, train[:, f])\n",
    "        #     if gain > best_gain:\n",
    "        #         best_gain = gain\n",
    "        #         best_feat = f\n",
    "        #         best_thr = threshold\n",
    "        # return best_feat, best_thr, best_gain\n",
    "        n_samples, n_features = train.shape\n",
    "        best_gain = -np.inf\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        if self.rf > 0:\n",
    "            n_sub = max(1, int(self.rf * n_features))\n",
    "            feature_indices = np.random.choice(n_features, n_sub, replace=False)\n",
    "        else:\n",
    "            feature_indices = range(n_features)\n",
    "        args_list = [(train[:, i], g, h) for i in feature_indices]\n",
    "        if self.multiproc and self.n_threads > 1:\n",
    "            with Pool(self.n_threads) as pool:\n",
    "                results = pool.map(self._threshold_wrapper, args_list)\n",
    "        else:\n",
    "            results = map(self._threshold_wrapper, args_list)\n",
    "        for feat_idx, (thr, gain) in zip(feature_indices, results):\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feat_idx\n",
    "                best_threshold = thr\n",
    "\n",
    "        return best_feature, best_threshold, best_gain\n",
    "    \n",
    "    def _threshold_wrapper(self, args):\n",
    "        # 这时 args = (feature_col, g, h)\n",
    "        feat_col, g, h = args\n",
    "        return self.find_threshold(g, h, feat_col)\n",
    "\n",
    "    def find_threshold(self, g, h, feature_col):\n",
    "        \"\"\"\n",
    "        Given gradients g, hessians h and a single feature column,\n",
    "        return (best_threshold, best_gain). Implements zero‐division checks,\n",
    "        min_sample_split enforcement, and the correct gain formula.\n",
    "        \"\"\"\n",
    "        # Sort by feature value\n",
    "        idx = np.argsort(feature_col)\n",
    "        f = feature_col[idx]\n",
    "        g_sorted = g[idx]\n",
    "        h_sorted = h[idx]\n",
    "\n",
    "        # Precompute totals\n",
    "        G_total = g_sorted.sum()\n",
    "        H_total = h_sorted.sum()\n",
    "        parent_term = G_total**2 / (H_total + self.lamda)\n",
    "\n",
    "        best_gain = -np.inf\n",
    "        best_thr = None\n",
    "\n",
    "        # Running sums for left node\n",
    "        G_left = 0.0\n",
    "        H_left = 0.0\n",
    "        n = len(f)\n",
    "\n",
    "        for i in range(n - 1):\n",
    "            G_left += g_sorted[i]\n",
    "            H_left += h_sorted[i]\n",
    "            G_right = G_total - G_left\n",
    "            H_right = H_total - H_left\n",
    "\n",
    "            # Skip if feature values are identical (no split)\n",
    "            if f[i] == f[i + 1]:\n",
    "                continue\n",
    "\n",
    "            # Enforce minimum samples in each split\n",
    "            if (i + 1) < self.min_sample_split or (n - i - 1) < self.min_sample_split:\n",
    "                continue\n",
    "\n",
    "            # Compute denominators, skip if non‐positive\n",
    "            den_left  = H_left  + self.lamda\n",
    "            den_right = H_right + self.lamda\n",
    "            if den_left <= 0 or den_right <= 0:\n",
    "                continue\n",
    "\n",
    "            # Gain = improvement over parent minus gamma\n",
    "            gain = (G_left**2  / den_left +\n",
    "                    G_right**2 / den_right -\n",
    "                    parent_term) - self.gamma\n",
    "\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_thr = (f[i] + f[i + 1]) / 2.0\n",
    "\n",
    "        if best_thr is None:\n",
    "            return None, 0.0\n",
    "        return best_thr, best_gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: class of Random Forest\n",
    "class RF(object):\n",
    "    '''\n",
    "    Class of Random Forest\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth d_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        rf = 0.99, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "        self.loss = leastsquare() if loss == 'mse' else logistic()\n",
    "        self.is_classification = True if loss != 'mse' else False\n",
    "\n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        #TODO\n",
    "        # Initialize trees\n",
    "        self.trees = [Tree(n_threads=self.n_threads, \n",
    "                           max_depth=self.max_depth, \n",
    "                           min_sample_split=self.min_sample_split,\n",
    "                           lamda=self.lamda, \n",
    "                           gamma=self.gamma, \n",
    "                           rf=self.rf) for _ in range(self.num_trees)]\n",
    "        # Fit each tree\n",
    "        for tree in self.trees:\n",
    "            # Calculate gradients and hessians\n",
    "            score = np.zeros(target.shape)\n",
    "            idx = np.random.choice(np.arange(train.shape[0]), train.shape[0], replace=True)\n",
    "            X_sample, y_sample = train[idx, :], target[idx]\n",
    "            g, h = self.loss.g(y_sample, score), self.loss.h(y_sample, score)\n",
    "            tree.fit(X_sample, g, h)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        raw = np.vstack([tree.predict(X) for tree in self.trees])\n",
    "        if self.is_classification:\n",
    "            score = raw.mean(axis=0)\n",
    "            return self.loss.pred(score)\n",
    "        else:\n",
    "            return raw.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluation functions (you can use code from previous homeworks)\n",
    "\n",
    "# RMSE\n",
    "def root_mean_square_error(pred, y):\n",
    "    #TODO\n",
    "    n = pred.shape[0]\n",
    "    return np.sqrt(np.sum((pred - y) ** 2) / n)\n",
    "    \n",
    "\n",
    "# precision\n",
    "def accuracy(pred, y):\n",
    "    #TODO\n",
    "    n = pred.shape[0]\n",
    "    return np.sum(pred == y) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: class of GBDT\n",
    "class GBDT(object):\n",
    "    '''\n",
    "    Class of gradient boosting decision tree (GBDT)\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth D_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        learning_rate: The learning rate eta of GBDT.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        learning_rate = 0.1, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = leastsquare() if loss == 'mse' else logistic()\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_trees = num_trees\n",
    "        self.is_classification = True if loss != 'mse' else False\n",
    "\n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        #TODO\n",
    "        n_samples, n_features = train.shape\n",
    "        self.trees = []\n",
    "        self.train = train\n",
    "        self.target = target\n",
    "        self.score = np.zeros(n_samples)\n",
    "        for _ in range(self.num_trees):\n",
    "            # Calculate gradients and hessians\n",
    "            g = self.loss.g(target, self.score)\n",
    "            h = self.loss.h(target, self.score)\n",
    "            # Fit a tree to the gradients and hessians\n",
    "            tree = Tree(n_threads=self.n_threads, \n",
    "                        max_depth=self.max_depth, \n",
    "                        min_sample_split=self.min_sample_split,\n",
    "                        lamda=self.lamda, \n",
    "                        gamma=self.gamma, rf=0)\n",
    "            tree.fit(train, g, h)\n",
    "            # Update the score with the predictions from the tree\n",
    "            self.score += self.learning_rate * tree.predict(train)\n",
    "            # Store the tree\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, test):\n",
    "        score = np.zeros(test.shape[0])\n",
    "        if self.is_classification:\n",
    "            for tree in self.trees:\n",
    "                score += self.learning_rate * tree.predict(test)\n",
    "            # 如果 loss.pred 只做 sigmoid → 0~1 概率：\n",
    "            proba = self.loss.pred(score)\n",
    "            return (proba > 0.5).astype(int)\n",
    "        else:\n",
    "            for tree in self.trees:\n",
    "                score += self.learning_rate * tree.predict(test)\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/1l/_31fxbps5_db7ymc5ghngxz00000gn/T/ipykernel_72235/289120647.py:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,) (354, 13) (354,) (152, 13) (152,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT regression on boston house price dataset\n",
    "\n",
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 3.172882841926914 | test rmse: 3.8463298122461476\n"
     ]
    }
   ],
   "source": [
    "# fit housing price with Random Forest\n",
    "model = RF(num_trees = 100, rf = 0.5, max_depth=10, min_sample_split=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred_train = model.predict(X_train)\n",
    "rmse_train = root_mean_square_error(pred_train, y_train)\n",
    "\n",
    "pred_test = model.predict(X_test)\n",
    "rmse_test = root_mean_square_error(pred_test, y_test)\n",
    "\n",
    "print('train rmse: {} | test rmse: {}'.format(rmse_train, rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT train rmse: 1.5963580003353244 | GBDT test rmse: 3.587045129457542\n"
     ]
    }
   ],
   "source": [
    "# fit housing price with GBDT\n",
    "model_gbdt = GBDT(\n",
    "    n_threads= 1,\n",
    "    loss='mse',\n",
    "    max_depth=5,\n",
    "    min_sample_split=10,\n",
    "    lamda=1,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    num_trees=50,\n",
    ")\n",
    "\n",
    "model_gbdt.fit(X_train, y_train)\n",
    "pred_train_gbdt = model_gbdt.predict(X_train)\n",
    "rmse_train_gbdt = root_mean_square_error(pred_train_gbdt, y_train)\n",
    "pred_test_gbdt = model_gbdt.predict(X_test)\n",
    "rmse_test_gbdt = root_mean_square_error(pred_test_gbdt, y_test)\n",
    "print('GBDT train rmse: {} | GBDT test rmse: {}'.format(rmse_train_gbdt, rmse_test_gbdt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 4.820626531838222 | test rmse: 5.209217510530889\n"
     ]
    }
   ],
   "source": [
    "# fit housing price with linear regression\n",
    "W = np.matmul(\n",
    "    np.linalg.inv(np.matmul(X_train.T, X_train)),\n",
    "    np.matmul(X_train.T, y_train))\n",
    "\n",
    "\n",
    "pred_train = np.matmul(X_train, W)\n",
    "rmse_train = root_mean_square_error(pred_train, y_train)\n",
    "\n",
    "pred_test = np.matmul(X_test, W)\n",
    "rmse_test = root_mean_square_error(pred_test, y_test)\n",
    "\n",
    "print('train rmse: {} | test rmse: {}'.format(rmse_train, rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 4.636494716898294 | test rmse: 4.9243576920779315\n"
     ]
    }
   ],
   "source": [
    "# fit housing price with ridge regression \n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_model = Ridge(alpha=0.5)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "pred_train = ridge_model.predict(X_train)\n",
    "rmse_train = root_mean_square_error(pred_train, y_train)\n",
    "pred_test = ridge_model.predict(X_test)\n",
    "rmse_test = root_mean_square_error(pred_test, y_test)\n",
    "print('train rmse: {} | test rmse: {}'.format(rmse_train, rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 61) (1000,) (700, 61) (700,) (300, 61) (300,)\n",
      "GBDT train acc: 0.8914285714285715 | GBDT test acc: 0.7633333333333333\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT classification on credit-g dataset\n",
    "\n",
    "# load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/')\n",
    "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))\n",
    "\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "X = X.values\n",
    "\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# fit credit-g with GBDT \n",
    "model_gbdt = GBDT(\n",
    "    n_threads=8,\n",
    "    loss='log',\n",
    "    max_depth=4,\n",
    "    min_sample_split=10,\n",
    "    lamda=1,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    num_trees=50\n",
    ")\n",
    "model_gbdt.fit(X_train, y_train)\n",
    "pred_train_gbdt = model_gbdt.predict(X_train)\n",
    "acc_train_gbdt = accuracy(pred_train_gbdt, y_train)\n",
    "pred_test_gbdt = model_gbdt.predict(X_test)\n",
    "acc_test_gbdt = accuracy(pred_test_gbdt, y_test)\n",
    "print('GBDT train acc: {} | GBDT test acc: {}'.format(acc_train_gbdt, acc_test_gbdt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Accuracy (Credit-g): 0.8385714285714285\n",
      "Random Forest Test Accuracy (Credit-g): 0.7466666666666667\n"
     ]
    }
   ],
   "source": [
    "# RF on credit-g\n",
    "rf_model = RF(\n",
    "    loss=logistic(),\n",
    "    max_depth=5,\n",
    "    min_sample_split=5,\n",
    "    lamda=1.5,\n",
    "    gamma=0,\n",
    "    rf=0.7,\n",
    "    num_trees=100\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train_prob = rf_model.predict(X_train)\n",
    "y_pred_test_prob = rf_model.predict(X_test)\n",
    "\n",
    "y_pred_train = (y_pred_train_prob > 0.5).astype(int)\n",
    "y_pred_test = (y_pred_test_prob > 0.5).astype(int)\n",
    "\n",
    "train_accuracy = accuracy(y_pred_train, y_train)\n",
    "test_accuracy = accuracy(y_pred_test, y_test)\n",
    "\n",
    "print(f\"Random Forest Training Accuracy (Credit-g): {train_accuracy}\")\n",
    "print(f\"Random Forest Test Accuracy (Credit-g): {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,) (398, 30) (398,) (171, 30) (171,)\n",
      "GBDT train acc: 0.9974874371859297 | GBDT test acc: 0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT classification on breast cancer dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# X = pd.get_dummies(X)\n",
    "# X = X.values\n",
    "# fit breast cancer with GBDT\n",
    "model_gbdt = GBDT(\n",
    "    n_threads=8,\n",
    "    loss='log',\n",
    "    max_depth=3,\n",
    "    min_sample_split=10,\n",
    "    lamda=1,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    num_trees=50\n",
    ")\n",
    "model_gbdt.fit(X_train, y_train)\n",
    "pred_train_gbdt = model_gbdt.predict(X_train)\n",
    "acc_train_gbdt = accuracy(pred_train_gbdt, y_train)\n",
    "pred_test_gbdt = model_gbdt.predict(X_test)\n",
    "acc_test_gbdt = accuracy(pred_test_gbdt, y_test)\n",
    "print('GBDT train acc: {} | GBDT test acc: {}'.format(acc_train_gbdt, acc_test_gbdt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Accuracy (breast cancer): 0.9798994974874372\n",
      "Random Forest Test Accuracy (breast cancer): 0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "# classify breast cancer with RF\n",
    "rf_model = RF(\n",
    "    loss=logistic(),\n",
    "    max_depth=10,\n",
    "    min_sample_split=5,\n",
    "    lamda=2,\n",
    "    gamma=1,\n",
    "    rf=0.7,\n",
    "    num_trees=100,\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train_prob = rf_model.predict(X_train)\n",
    "y_pred_test_prob = rf_model.predict(X_test)\n",
    "\n",
    "y_pred_train = (y_pred_train_prob > 0.5).astype(int)\n",
    "y_pred_test = (y_pred_test_prob > 0.5).astype(int)\n",
    "\n",
    "train_accuracy = accuracy(y_pred_train, y_train)\n",
    "test_accuracy = accuracy(y_pred_test, y_test)\n",
    "\n",
    "print(f\"Random Forest Training Accuracy (breast cancer): {train_accuracy}\")\n",
    "print(f\"Random Forest Test Accuracy (breast cancer): {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
